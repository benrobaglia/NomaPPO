{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d853ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "576b920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.noma_ppo import *\n",
    "from nomaenv import NomaEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0f727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af5ce2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 12\n",
    "deadlines = np.array([5 for _ in range(k)])\n",
    "offsets = None\n",
    "arrival_probs = None\n",
    "period = None\n",
    "\n",
    "env = NomaEnv(k,\n",
    "              deadlines,\n",
    "              lbda=1/9.3,\n",
    "              period=period,\n",
    "              arrival_probs=arrival_probs,\n",
    "              offsets=offsets,\n",
    "              episode_length=200,\n",
    "              max_simultaneous_devices=3,\n",
    "              traffic_model='aperiodic',\n",
    "              channel_model='collision',\n",
    "              distances=None,\n",
    "              path_loss=False,\n",
    "              shadowing=False,\n",
    "              fast_fading=False,\n",
    "              verbose=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae6f534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nomappo = NomaPPO(env, \n",
    "                 lr_actor=1e-4,\n",
    "                 lr_critic=1e-3,\n",
    "                 hidden_size=128,\n",
    "                 gamma=0.4,\n",
    "                 K_epochs=4,\n",
    "                 eps_clip=0.1,\n",
    "                 prior_weight=0.6,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d005d620",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5256325450636679"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomappo.test(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68e8676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 0, Reward : 0.995, Train scores: 0.7991967871485943, Test score : 0.5474837662337662, policy lr: 0.0001\n",
      "Episode : 20, Reward : 0.99775, Train scores: 0.8134027202135364, Test score : 0.5631385369840621, policy lr: 0.0001\n",
      "Episode : 40, Reward : 0.975, Train scores: 0.8188276811888896, Test score : 0.5835777126099706, policy lr: 0.0001\n",
      "Episode : 60, Reward : 0.99775, Train scores: 0.8451440164190689, Test score : 0.6063059224541969, policy lr: 0.0001\n",
      "Episode : 80, Reward : 1.01125, Train scores: 0.847156014797744, Test score : 0.6034691407825736, policy lr: 0.0001\n",
      "Episode : 100, Reward : 1.0145, Train scores: 0.8462250357950735, Test score : 0.653416149068323, policy lr: 0.0001\n",
      "Episode : 120, Reward : 1.03575, Train scores: 0.8756571531806833, Test score : 0.6761433868974043, policy lr: 0.0001\n",
      "Episode : 140, Reward : 1.0275, Train scores: 0.8763097430035918, Test score : 0.6794975688816856, policy lr: 0.0001\n",
      "Episode : 160, Reward : 1.027, Train scores: 0.8775430810984727, Test score : 0.7163179916317992, policy lr: 0.0001\n",
      "Episode : 180, Reward : 1.0545, Train scores: 0.8866037689572208, Test score : 0.7339797639123102, policy lr: 0.0001\n",
      "Episode : 200, Reward : 1.0855, Train scores: 0.9043762977428706, Test score : 0.7255054432348367, policy lr: 0.0001\n",
      "Episode : 220, Reward : 1.07175, Train scores: 0.9154668284558488, Test score : 0.7473382473382473, policy lr: 0.0001\n",
      "Episode : 240, Reward : 1.06725, Train scores: 0.9174388833311957, Test score : 0.8236266005782734, policy lr: 0.0001\n",
      "Episode : 260, Reward : 1.10125, Train scores: 0.9161960020123928, Test score : 0.7956521739130434, policy lr: 0.0001\n",
      "Episode : 280, Reward : 1.097, Train scores: 0.9342489903309463, Test score : 0.8339350180505415, policy lr: 0.0001\n",
      "Episode : 300, Reward : 1.10475, Train scores: 0.927240091026011, Test score : 0.8488278092158448, policy lr: 0.0001\n",
      "Episode : 320, Reward : 1.10425, Train scores: 0.9265374558002195, Test score : 0.8577170418006431, policy lr: 0.0001\n",
      "Episode : 340, Reward : 1.11675, Train scores: 0.9332574211009341, Test score : 0.8600083577099875, policy lr: 0.0001\n",
      "Episode : 360, Reward : 1.10875, Train scores: 0.9406878102499527, Test score : 0.8822567457072772, policy lr: 0.0001\n",
      "Episode : 380, Reward : 1.121, Train scores: 0.944385062517427, Test score : 0.8885245901639345, policy lr: 0.0001\n",
      "Episode : 400, Reward : 1.141, Train scores: 0.9433572785324336, Test score : 0.9142976588628763, policy lr: 0.0001\n",
      "Episode : 420, Reward : 1.11075, Train scores: 0.9552264982974098, Test score : 0.8535291717666259, policy lr: 0.0001\n",
      "Episode : 440, Reward : 1.15625, Train scores: 0.960842309217709, Test score : 0.9184426229508197, policy lr: 0.0001\n",
      "Episode : 460, Reward : 1.13475, Train scores: 0.9545987307406024, Test score : 0.835428122545169, policy lr: 0.0001\n",
      "Episode : 480, Reward : 1.13875, Train scores: 0.9570446148598484, Test score : 0.9326963024511841, policy lr: 0.0001\n",
      "Episode : 500, Reward : 1.1515, Train scores: 0.9534083560321612, Test score : 0.89652448657188, policy lr: 0.0001\n",
      "Episode : 520, Reward : 1.13775, Train scores: 0.954535474625531, Test score : 0.9426934097421203, policy lr: 0.0001\n",
      "Episode : 540, Reward : 1.112, Train scores: 0.9580001341240834, Test score : 0.8539192399049882, policy lr: 0.0001\n",
      "Episode : 560, Reward : 1.1085, Train scores: 0.9596963311246884, Test score : 0.8850085178875639, policy lr: 0.0001\n",
      "Episode : 580, Reward : 1.15, Train scores: 0.9555640351123204, Test score : 0.9303399076793957, policy lr: 0.0001\n",
      "Episode : 600, Reward : 1.12475, Train scores: 0.9514028350426027, Test score : 0.8672064777327935, policy lr: 0.0001\n",
      "Episode : 620, Reward : 1.149, Train scores: 0.9589100203327311, Test score : 0.9189991518235793, policy lr: 0.0001\n",
      "Episode : 640, Reward : 1.15425, Train scores: 0.952822917385778, Test score : 0.8935996738687322, policy lr: 0.0001\n",
      "Episode : 660, Reward : 1.1335, Train scores: 0.9591402809630851, Test score : 0.8749000799360511, policy lr: 0.0001\n",
      "Episode : 680, Reward : 1.12875, Train scores: 0.9518154696231541, Test score : 0.933817594834544, policy lr: 0.0001\n",
      "Episode : 700, Reward : 1.1455, Train scores: 0.9609366862383422, Test score : 0.8540497193263833, policy lr: 0.0001\n",
      "Episode : 720, Reward : 1.11575, Train scores: 0.9544383893437007, Test score : 0.9175642087821044, policy lr: 0.0001\n",
      "Episode : 740, Reward : 1.1245, Train scores: 0.9579576409953685, Test score : 0.9432884536923705, policy lr: 0.0001\n",
      "Episode : 760, Reward : 1.1535, Train scores: 0.9671146522936794, Test score : 0.934656741108354, policy lr: 0.0001\n",
      "Episode : 780, Reward : 1.15975, Train scores: 0.9630887788582759, Test score : 0.9377759935768768, policy lr: 0.0001\n",
      "Episode : 800, Reward : 1.121, Train scores: 0.9607678683253891, Test score : 0.9045848822800495, policy lr: 0.0001\n",
      "Episode : 820, Reward : 1.14675, Train scores: 0.9566283313689098, Test score : 0.9371196754563894, policy lr: 0.0001\n",
      "Episode : 840, Reward : 1.15275, Train scores: 0.9618408385314003, Test score : 0.8995650454725188, policy lr: 0.0001\n",
      "Episode : 860, Reward : 1.13125, Train scores: 0.9654839715555854, Test score : 0.9282511210762332, policy lr: 0.0001\n",
      "Episode : 880, Reward : 1.129, Train scores: 0.9629559075227802, Test score : 0.9419809365934522, policy lr: 0.0001\n",
      "Episode : 900, Reward : 1.14775, Train scores: 0.9584228988221467, Test score : 0.9418960244648318, policy lr: 0.0001\n",
      "Episode : 920, Reward : 1.1365, Train scores: 0.9636414044923282, Test score : 0.9621797478649857, policy lr: 0.0001\n",
      "Episode : 940, Reward : 1.14825, Train scores: 0.9576700970107271, Test score : 0.9560394412489729, policy lr: 0.0001\n",
      "Episode : 960, Reward : 1.156, Train scores: 0.9656354417790445, Test score : 0.9552481692432873, policy lr: 0.0001\n",
      "Episode : 980, Reward : 1.13625, Train scores: 0.952778648177833, Test score : 0.9306613226452906, policy lr: 0.0001\n",
      "Episode : 1000, Reward : 1.13275, Train scores: 0.9617563170466527, Test score : 0.9582329317269076, policy lr: 0.0001\n",
      "Episode : 1020, Reward : 1.1215, Train scores: 0.9606601423339368, Test score : 0.9443990188062142, policy lr: 0.0001\n",
      "Episode : 1040, Reward : 1.1605, Train scores: 0.9631165618379669, Test score : 0.932570977917981, policy lr: 0.0001\n",
      "Episode : 1060, Reward : 1.14925, Train scores: 0.9568687158747572, Test score : 0.9236082893132873, policy lr: 0.0001\n",
      "Episode : 1080, Reward : 1.143, Train scores: 0.9566272610375617, Test score : 0.9310344827586207, policy lr: 0.0001\n",
      "Episode : 1100, Reward : 1.1365, Train scores: 0.9597765451815021, Test score : 0.9277456647398844, policy lr: 0.0001\n",
      "Episode : 1120, Reward : 1.14675, Train scores: 0.9590942939614484, Test score : 0.9236139630390143, policy lr: 0.0001\n",
      "Episode : 1140, Reward : 1.14775, Train scores: 0.9669040293258142, Test score : 0.9494470774091627, policy lr: 0.0001\n",
      "Episode : 1160, Reward : 1.157, Train scores: 0.9640647036826142, Test score : 0.9187024429315178, policy lr: 0.0001\n",
      "Episode : 1180, Reward : 1.14625, Train scores: 0.9627217218767103, Test score : 0.8940558026688232, policy lr: 0.0001\n",
      "Episode : 1200, Reward : 1.096, Train scores: 0.9565974752548522, Test score : 0.9322503961965135, policy lr: 0.0001\n",
      "Episode : 1220, Reward : 1.11, Train scores: 0.9536740506427908, Test score : 0.9355918025930573, policy lr: 0.0001\n",
      "Episode : 1240, Reward : 1.14625, Train scores: 0.9570674315816726, Test score : 0.901090028259992, policy lr: 0.0001\n",
      "Episode : 1260, Reward : 1.13375, Train scores: 0.960987579603539, Test score : 0.9717213114754099, policy lr: 0.0001\n",
      "Episode : 1280, Reward : 1.1405, Train scores: 0.9626703212858232, Test score : 0.9192166462668299, policy lr: 0.0001\n",
      "Episode : 1300, Reward : 1.14425, Train scores: 0.9581568734934336, Test score : 0.9436392914653784, policy lr: 0.0001\n",
      "Episode : 1320, Reward : 1.13125, Train scores: 0.9616785648942301, Test score : 0.9128329297820823, policy lr: 0.0001\n",
      "Episode : 1340, Reward : 1.17525, Train scores: 0.9717700862760557, Test score : 0.9270103092783505, policy lr: 0.0001\n",
      "Episode : 1360, Reward : 1.16475, Train scores: 0.9593930989631136, Test score : 0.9335507541785568, policy lr: 0.0001\n",
      "Episode : 1380, Reward : 1.12975, Train scores: 0.9635345903613327, Test score : 0.9626009349766256, policy lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1400, Reward : 1.1455, Train scores: 0.9554792388973737, Test score : 0.9541359541359541, policy lr: 0.0001\n",
      "Episode : 1420, Reward : 1.09275, Train scores: 0.9549524518634831, Test score : 0.9688498402555911, policy lr: 0.0001\n",
      "Episode : 1440, Reward : 1.13875, Train scores: 0.9571547427962482, Test score : 0.9757966922146026, policy lr: 0.0001\n",
      "Episode : 1460, Reward : 1.13625, Train scores: 0.9638995033486963, Test score : 0.9181708784596871, policy lr: 0.0001\n",
      "Episode : 1480, Reward : 1.1085, Train scores: 0.9602565970193787, Test score : 0.9496433067561897, policy lr: 0.0001\n",
      "Episode : 1500, Reward : 1.141, Train scores: 0.9656534356513268, Test score : 0.9637243047158404, policy lr: 0.0001\n",
      "Episode : 1520, Reward : 1.112, Train scores: 0.9578598635428024, Test score : 0.9201171058134672, policy lr: 0.0001\n",
      "Episode : 1540, Reward : 1.10625, Train scores: 0.9664336376449434, Test score : 0.9460016488046167, policy lr: 0.0001\n",
      "Episode : 1560, Reward : 1.13025, Train scores: 0.968212411753474, Test score : 0.9428571428571428, policy lr: 0.0001\n",
      "Episode : 1580, Reward : 1.1575, Train scores: 0.9631513595831154, Test score : 0.9382866423061307, policy lr: 0.0001\n",
      "Episode : 1600, Reward : 1.1375, Train scores: 0.9685962852368073, Test score : 0.919826224328594, policy lr: 0.0001\n",
      "Episode : 1620, Reward : 1.143, Train scores: 0.9609713693289119, Test score : 0.9180129240710824, policy lr: 0.0001\n",
      "Episode : 1640, Reward : 1.14825, Train scores: 0.9621515637596276, Test score : 0.9353274050121261, policy lr: 0.0001\n",
      "Episode : 1660, Reward : 1.14625, Train scores: 0.9671943957722569, Test score : 0.9364069952305246, policy lr: 0.0001\n",
      "Episode : 1680, Reward : 1.10675, Train scores: 0.9504705792976452, Test score : 0.9568658488326078, policy lr: 0.0001\n",
      "Episode : 1700, Reward : 1.13, Train scores: 0.9565391082527427, Test score : 0.9495319495319495, policy lr: 0.0001\n",
      "Episode : 1720, Reward : 1.13125, Train scores: 0.9559477030481904, Test score : 0.9735720375106565, policy lr: 0.0001\n",
      "Episode : 1740, Reward : 1.15275, Train scores: 0.9694641088151217, Test score : 0.9373464373464373, policy lr: 0.0001\n",
      "Episode : 1760, Reward : 1.13625, Train scores: 0.9668635624032967, Test score : 0.9537664408130729, policy lr: 0.0001\n",
      "Episode : 1780, Reward : 1.162, Train scores: 0.9676394584739209, Test score : 0.9624183006535948, policy lr: 0.0001\n",
      "Episode : 1800, Reward : 1.1655, Train scores: 0.9675275647672776, Test score : 0.9499198717948718, policy lr: 0.0001\n",
      "Episode : 1820, Reward : 1.144, Train scores: 0.9671219789021995, Test score : 0.9464642429085098, policy lr: 0.0001\n",
      "Episode : 1840, Reward : 1.1505, Train scores: 0.9620063262081905, Test score : 0.9514285714285714, policy lr: 0.0001\n",
      "Episode : 1860, Reward : 1.1615, Train scores: 0.9564053621441977, Test score : 0.950210970464135, policy lr: 0.0001\n",
      "Episode : 1880, Reward : 1.1235, Train scores: 0.9591807748944753, Test score : 0.9739307535641548, policy lr: 0.0001\n",
      "Episode : 1900, Reward : 1.14625, Train scores: 0.9641249760640977, Test score : 0.9520222045995241, policy lr: 0.0001\n",
      "Episode : 1920, Reward : 1.16175, Train scores: 0.9600124938386463, Test score : 0.9727610400330169, policy lr: 0.0001\n",
      "Episode : 1940, Reward : 1.164, Train scores: 0.9619936908599515, Test score : 0.9609186140209508, policy lr: 0.0001\n",
      "Episode : 1960, Reward : 1.11875, Train scores: 0.9549986534661686, Test score : 0.9484960857025134, policy lr: 0.0001\n",
      "Episode : 1980, Reward : 1.129, Train scores: 0.9690380761011484, Test score : 0.9710085749285423, policy lr: 0.0001\n",
      "Episode : 2000, Reward : 1.153, Train scores: 0.9721024387339112, Test score : 0.9711659005432511, policy lr: 0.0001\n",
      "Episode : 2020, Reward : 1.13675, Train scores: 0.9547256218335216, Test score : 0.9467382328654005, policy lr: 0.0001\n",
      "Episode : 2040, Reward : 1.13525, Train scores: 0.9597012105674103, Test score : 0.9632231404958678, policy lr: 0.0001\n",
      "Episode : 2060, Reward : 1.144, Train scores: 0.96212773208025, Test score : 0.9354183590576767, policy lr: 0.0001\n",
      "Episode : 2080, Reward : 1.132, Train scores: 0.9638103411470553, Test score : 0.9509326845093269, policy lr: 0.0001\n",
      "Episode : 2100, Reward : 1.148, Train scores: 0.962193591626594, Test score : 0.9755511022044088, policy lr: 0.0001\n",
      "Episode : 2120, Reward : 1.13525, Train scores: 0.9627608725636883, Test score : 0.985, policy lr: 0.0001\n",
      "Episode : 2140, Reward : 1.13775, Train scores: 0.9655830958539081, Test score : 0.9353212935741285, policy lr: 0.0001\n",
      "Episode : 2160, Reward : 1.15125, Train scores: 0.9625768557592972, Test score : 0.9162345432788193, policy lr: 0.0001\n",
      "Episode : 2180, Reward : 1.14775, Train scores: 0.9721000853749471, Test score : 0.9405695418902187, policy lr: 0.0001\n",
      "Episode : 2200, Reward : 1.14925, Train scores: 0.9763699748177317, Test score : 0.9644752960391997, policy lr: 0.0001\n",
      "Episode : 2220, Reward : 1.15675, Train scores: 0.9661512103087778, Test score : 0.9247011952191235, policy lr: 0.0001\n",
      "Episode : 2240, Reward : 1.14025, Train scores: 0.968139301614592, Test score : 0.9741235392320534, policy lr: 0.0001\n",
      "Episode : 2260, Reward : 1.14525, Train scores: 0.96610495814079, Test score : 0.9379875195007801, policy lr: 0.0001\n",
      "Episode : 2280, Reward : 1.156, Train scores: 0.9601459668187908, Test score : 0.9437525110486139, policy lr: 0.0001\n",
      "Episode : 2300, Reward : 1.13025, Train scores: 0.9614996011918617, Test score : 0.9527494908350306, policy lr: 0.0001\n",
      "Episode : 2320, Reward : 1.13625, Train scores: 0.9638249078522655, Test score : 0.9741035856573705, policy lr: 0.0001\n",
      "Episode : 2340, Reward : 1.11725, Train scores: 0.9615496102772807, Test score : 0.9635459817729909, policy lr: 0.0001\n",
      "Episode : 2360, Reward : 1.122, Train scores: 0.9622399699401738, Test score : 0.9804321239298818, policy lr: 0.0001\n",
      "Episode : 2380, Reward : 1.16575, Train scores: 0.9602220143312804, Test score : 0.9681818181818181, policy lr: 0.0001\n",
      "Episode : 2400, Reward : 1.15475, Train scores: 0.9603342793910306, Test score : 0.9533468559837728, policy lr: 0.0001\n",
      "Episode : 2420, Reward : 1.12375, Train scores: 0.9541031352618845, Test score : 0.9557809330628804, policy lr: 0.0001\n",
      "Episode : 2440, Reward : 1.129, Train scores: 0.9609380110551413, Test score : 0.9232006433453961, policy lr: 0.0001\n",
      "Episode : 2460, Reward : 1.135, Train scores: 0.9598594641176419, Test score : 0.9694625407166124, policy lr: 0.0001\n",
      "Episode : 2480, Reward : 1.13575, Train scores: 0.9633731099193883, Test score : 0.9485355648535565, policy lr: 0.0001\n",
      "Episode : 2500, Reward : 1.1225, Train scores: 0.957564160833253, Test score : 0.9864291772688719, policy lr: 0.0001\n",
      "Episode : 2520, Reward : 1.14125, Train scores: 0.9654042284743902, Test score : 0.9925218113834648, policy lr: 0.0001\n",
      "Episode : 2540, Reward : 1.15475, Train scores: 0.9705863032223782, Test score : 0.9717959352965575, policy lr: 0.0001\n",
      "Episode : 2560, Reward : 1.14525, Train scores: 0.9570986567625592, Test score : 0.9468988954970263, policy lr: 0.0001\n",
      "Episode : 2580, Reward : 1.143, Train scores: 0.9597738743723992, Test score : 0.972939729397294, policy lr: 0.0001\n",
      "Episode : 2600, Reward : 1.137, Train scores: 0.9606186885634893, Test score : 0.9378624689312345, policy lr: 0.0001\n",
      "Episode : 2620, Reward : 1.152, Train scores: 0.9676948273650607, Test score : 0.9622799664710813, policy lr: 0.0001\n",
      "Episode : 2640, Reward : 1.12975, Train scores: 0.9632826802019187, Test score : 0.9337564458548195, policy lr: 0.0001\n",
      "Episode : 2660, Reward : 1.1285, Train scores: 0.9606023456623903, Test score : 0.9504301515772224, policy lr: 0.0001\n",
      "Episode : 2680, Reward : 1.12675, Train scores: 0.9623528794642716, Test score : 0.9604887983706721, policy lr: 0.0001\n",
      "Episode : 2700, Reward : 1.13725, Train scores: 0.9710221927986593, Test score : 0.9785743716522456, policy lr: 0.0001\n",
      "Episode : 2720, Reward : 1.13775, Train scores: 0.9631400941224879, Test score : 0.9790897908979089, policy lr: 0.0001\n",
      "Episode : 2740, Reward : 1.1365, Train scores: 0.9570835028830482, Test score : 0.9655458696554587, policy lr: 0.0001\n",
      "Episode : 2760, Reward : 1.11725, Train scores: 0.9542469063576181, Test score : 0.9688269073010665, policy lr: 0.0001\n",
      "Episode : 2780, Reward : 1.133, Train scores: 0.9663772988044135, Test score : 0.9734625105307498, policy lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 2800, Reward : 1.12075, Train scores: 0.9572177534483488, Test score : 0.9718426501035197, policy lr: 0.0001\n",
      "Episode : 2820, Reward : 1.134, Train scores: 0.9596890189727137, Test score : 0.9709090909090909, policy lr: 0.0001\n",
      "Episode : 2840, Reward : 1.13775, Train scores: 0.9606452902957455, Test score : 0.9592424866200082, policy lr: 0.0001\n",
      "Episode : 2860, Reward : 1.1445, Train scores: 0.9625328497928247, Test score : 0.9614767255216693, policy lr: 0.0001\n",
      "Episode : 2880, Reward : 1.11975, Train scores: 0.957421543573477, Test score : 0.9421182266009852, policy lr: 0.0001\n",
      "Episode : 2900, Reward : 1.1395, Train scores: 0.9586490078198672, Test score : 0.9656274980015987, policy lr: 0.0001\n",
      "Episode : 2920, Reward : 1.1455, Train scores: 0.9711014175273167, Test score : 0.9606741573033708, policy lr: 0.0001\n",
      "Episode : 2940, Reward : 1.12525, Train scores: 0.9625341111992087, Test score : 0.956997971602434, policy lr: 0.0001\n",
      "Episode : 2960, Reward : 1.164, Train scores: 0.971351440767352, Test score : 0.9570453633079085, policy lr: 0.0001\n",
      "Episode : 2980, Reward : 1.1275, Train scores: 0.9559766268264607, Test score : 0.9566787003610109, policy lr: 0.0001\n",
      "Episode : 3000, Reward : 1.12625, Train scores: 0.9638761963508051, Test score : 0.9679643146796432, policy lr: 0.0001\n",
      "Episode : 3020, Reward : 1.16475, Train scores: 0.9672908716313279, Test score : 0.9565936337329475, policy lr: 0.0001\n",
      "Episode : 3040, Reward : 1.117, Train scores: 0.9560422010682614, Test score : 0.9588762214983714, policy lr: 0.0001\n",
      "Episode : 3060, Reward : 1.13, Train scores: 0.96808563886628, Test score : 0.9673602611179111, policy lr: 0.0001\n",
      "Episode : 3080, Reward : 1.183, Train scores: 0.9776006888468339, Test score : 0.9740847387906212, policy lr: 0.0001\n",
      "Episode : 3100, Reward : 1.11525, Train scores: 0.9612557935452186, Test score : 0.9709784411276948, policy lr: 0.0001\n",
      "Episode : 3120, Reward : 1.15375, Train scores: 0.9605325456794107, Test score : 0.9360160965794768, policy lr: 0.0001\n",
      "Episode : 3140, Reward : 1.13, Train scores: 0.9616859477866108, Test score : 0.9851668726823238, policy lr: 0.0001\n",
      "Episode : 3160, Reward : 1.1355, Train scores: 0.9679333491737241, Test score : 0.9672991522002422, policy lr: 0.0001\n",
      "Episode : 3180, Reward : 1.16725, Train scores: 0.9716019863434175, Test score : 0.9703557312252964, policy lr: 0.0001\n",
      "Episode : 3200, Reward : 1.11425, Train scores: 0.9605188256477621, Test score : 0.9833536337799431, policy lr: 0.0001\n",
      "Episode : 3220, Reward : 1.15775, Train scores: 0.9709639653568898, Test score : 0.9667342799188641, policy lr: 0.0001\n",
      "Episode : 3240, Reward : 1.14075, Train scores: 0.9608261171267827, Test score : 0.9584533113944879, policy lr: 0.0001\n",
      "Episode : 3260, Reward : 1.163, Train scores: 0.969136299189547, Test score : 0.9807849550286182, policy lr: 0.0001\n",
      "Episode : 3280, Reward : 1.139, Train scores: 0.970870649387866, Test score : 0.9437929640113223, policy lr: 0.0001\n",
      "Episode : 3300, Reward : 1.14025, Train scores: 0.9646411005559413, Test score : 0.9490164592533119, policy lr: 0.0001\n",
      "Episode : 3320, Reward : 1.1305, Train scores: 0.9718814516978005, Test score : 0.9565746753246753, policy lr: 0.0001\n",
      "Episode : 3340, Reward : 1.12675, Train scores: 0.9607903233901327, Test score : 0.9639934533551555, policy lr: 0.0001\n",
      "Episode : 3360, Reward : 1.16475, Train scores: 0.9668450740004175, Test score : 0.9844472467423288, policy lr: 0.0001\n",
      "Episode : 3380, Reward : 1.1675, Train scores: 0.9596538759478568, Test score : 0.9581881533101045, policy lr: 0.0001\n",
      "Episode : 3400, Reward : 1.13625, Train scores: 0.9609540168927884, Test score : 0.9733333333333334, policy lr: 0.0001\n",
      "Episode : 3420, Reward : 1.12275, Train scores: 0.964737777688532, Test score : 0.9792020373514432, policy lr: 0.0001\n",
      "Episode : 3440, Reward : 1.129, Train scores: 0.9593285902596603, Test score : 0.9781970649895179, policy lr: 0.0001\n",
      "Episode : 3460, Reward : 1.1385, Train scores: 0.9610861041331367, Test score : 0.9755799755799756, policy lr: 0.0001\n",
      "Episode : 3480, Reward : 1.15725, Train scores: 0.9694647259514596, Test score : 0.9963310232368529, policy lr: 0.0001\n",
      "Episode : 3500, Reward : 1.15775, Train scores: 0.9699457317998172, Test score : 0.9795249795249795, policy lr: 0.0001\n",
      "Episode : 3520, Reward : 1.105, Train scores: 0.9564622495152969, Test score : 0.9522854851643946, policy lr: 0.0001\n",
      "Episode : 3540, Reward : 1.156, Train scores: 0.962199420290012, Test score : 0.9760302775441547, policy lr: 0.0001\n",
      "Episode : 3560, Reward : 1.17475, Train scores: 0.9719523211785439, Test score : 0.9553752535496958, policy lr: 0.0001\n",
      "Episode : 3580, Reward : 1.16375, Train scores: 0.9717038167647685, Test score : 0.9785743716522456, policy lr: 0.0001\n",
      "Episode : 3600, Reward : 1.1185, Train scores: 0.9706466919706473, Test score : 0.9592999592999593, policy lr: 0.0001\n",
      "Episode : 3620, Reward : 1.137, Train scores: 0.9689374999126352, Test score : 0.9771084337349397, policy lr: 0.0001\n",
      "Episode : 3640, Reward : 1.138, Train scores: 0.9519580300745247, Test score : 0.9876084262701363, policy lr: 0.0001\n",
      "Episode : 3660, Reward : 1.14275, Train scores: 0.9650766387649099, Test score : 0.9530874097834804, policy lr: 0.0001\n",
      "Episode : 3680, Reward : 1.17, Train scores: 0.9691262442763259, Test score : 0.951310861423221, policy lr: 0.0001\n",
      "Episode : 3700, Reward : 1.146, Train scores: 0.9696409942227942, Test score : 0.9808253438932889, policy lr: 0.0001\n",
      "Episode : 3720, Reward : 1.11075, Train scores: 0.9653460811444727, Test score : 0.9817654372150849, policy lr: 0.0001\n",
      "Episode : 3740, Reward : 1.13475, Train scores: 0.9671046959988931, Test score : 0.9794463087248322, policy lr: 0.0001\n",
      "Episode : 3760, Reward : 1.14275, Train scores: 0.9617698426257206, Test score : 0.9722108704536166, policy lr: 0.0001\n",
      "Episode : 3780, Reward : 1.1225, Train scores: 0.9554936608850847, Test score : 0.9609438567941416, policy lr: 0.0001\n",
      "Episode : 3800, Reward : 1.14525, Train scores: 0.9699830712697782, Test score : 0.9701733172108021, policy lr: 0.0001\n",
      "Episode : 3820, Reward : 1.13975, Train scores: 0.9649086116697247, Test score : 0.9706572769953051, policy lr: 0.0001\n",
      "Episode : 3840, Reward : 1.14975, Train scores: 0.972772315619266, Test score : 0.9819004524886877, policy lr: 0.0001\n",
      "Episode : 3860, Reward : 1.13675, Train scores: 0.9693110034537081, Test score : 0.9708582834331337, policy lr: 0.0001\n",
      "Episode : 3880, Reward : 1.1385, Train scores: 0.9628920645581186, Test score : 0.9895034315704481, policy lr: 0.0001\n",
      "Episode : 3900, Reward : 1.1155, Train scores: 0.9629408063120748, Test score : 0.976594027441485, policy lr: 0.0001\n",
      "Episode : 3920, Reward : 1.1185, Train scores: 0.9687996541036226, Test score : 0.9652665589660743, policy lr: 0.0001\n",
      "Episode : 3940, Reward : 1.16125, Train scores: 0.9721842318841605, Test score : 0.9739386296763346, policy lr: 0.0001\n",
      "Episode : 3960, Reward : 1.1335, Train scores: 0.9680296069224757, Test score : 0.9768403639371381, policy lr: 0.0001\n",
      "Episode : 3980, Reward : 1.165, Train scores: 0.961435751577285, Test score : 0.9650780608052588, policy lr: 0.0001\n",
      "Episode : 4000, Reward : 1.152, Train scores: 0.9708329089931473, Test score : 0.9609507640067911, policy lr: 0.0001\n",
      "Episode : 4020, Reward : 1.147, Train scores: 0.9659199943374823, Test score : 0.9953566905867455, policy lr: 0.0001\n",
      "Episode : 4040, Reward : 1.1395, Train scores: 0.9686872730040157, Test score : 0.9856902356902357, policy lr: 0.0001\n",
      "Episode : 4060, Reward : 1.164, Train scores: 0.9661126318305253, Test score : 0.9775551102204408, policy lr: 0.0001\n",
      "Episode : 4080, Reward : 1.17875, Train scores: 0.9725896515560748, Test score : 0.9743384121892542, policy lr: 0.0001\n",
      "Episode : 4100, Reward : 1.126, Train scores: 0.9545357247715996, Test score : 0.9809870550161812, policy lr: 0.0001\n",
      "Episode : 4120, Reward : 1.1415, Train scores: 0.9664758361857275, Test score : 0.9538021259198691, policy lr: 0.0001\n",
      "Episode : 4140, Reward : 1.13825, Train scores: 0.9744280083502875, Test score : 0.9798305768455022, policy lr: 0.0001\n",
      "Episode : 4160, Reward : 1.11375, Train scores: 0.960603445252703, Test score : 0.9396657154504688, policy lr: 0.0001\n",
      "Episode : 4180, Reward : 1.114, Train scores: 0.961709192093946, Test score : 0.9838836477987422, policy lr: 0.0001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 4200, Reward : 1.13975, Train scores: 0.9693798233969094, Test score : 0.9535076795350768, policy lr: 0.0001\n",
      "Episode : 4220, Reward : 1.13475, Train scores: 0.9711794669590171, Test score : 0.9783761729906161, policy lr: 0.0001\n",
      "Episode : 4240, Reward : 1.1425, Train scores: 0.9671331991294961, Test score : 0.96347757046447, policy lr: 0.0001\n",
      "Episode : 4260, Reward : 1.136, Train scores: 0.9715339606159672, Test score : 0.9931754315535929, policy lr: 0.0001\n",
      "Episode : 4280, Reward : 1.1305, Train scores: 0.9716622973587905, Test score : 0.9770491803278688, policy lr: 0.0001\n",
      "Episode : 4300, Reward : 1.153, Train scores: 0.9571831181877439, Test score : 0.9808631921824105, policy lr: 0.0001\n",
      "Episode : 4320, Reward : 1.15425, Train scores: 0.96677268309556, Test score : 0.9746572496884088, policy lr: 0.0001\n",
      "Episode : 4340, Reward : 1.14525, Train scores: 0.964558512122809, Test score : 0.9739625711960944, policy lr: 0.0001\n",
      "Episode : 4360, Reward : 1.14525, Train scores: 0.9670913801099061, Test score : 0.9748580697485807, policy lr: 0.0001\n",
      "Episode : 4380, Reward : 1.16275, Train scores: 0.9655371309511981, Test score : 0.9891621508962067, policy lr: 0.0001\n",
      "Episode : 4400, Reward : 1.1615, Train scores: 0.9655282212056076, Test score : 0.9920965058236273, policy lr: 0.0001\n",
      "Episode : 4420, Reward : 1.1455, Train scores: 0.9598604237501773, Test score : 0.9879767827529021, policy lr: 0.0001\n",
      "Episode : 4440, Reward : 1.151, Train scores: 0.9617019667430536, Test score : 0.9525547445255474, policy lr: 0.0001\n",
      "Episode : 4460, Reward : 1.13975, Train scores: 0.9684016802090755, Test score : 0.9687756778964667, policy lr: 0.0001\n",
      "Episode : 4480, Reward : 1.13825, Train scores: 0.9685763580514521, Test score : 0.9572, policy lr: 0.0001\n",
      "Episode : 4500, Reward : 1.15325, Train scores: 0.9724999046969197, Test score : 0.9347737464329393, policy lr: 0.0001\n",
      "Episode : 4520, Reward : 1.14625, Train scores: 0.9698438848534303, Test score : 0.9860655737704918, policy lr: 0.0001\n",
      "Episode : 4540, Reward : 1.14475, Train scores: 0.967866742995489, Test score : 0.99179991799918, policy lr: 0.0001\n",
      "Episode : 4560, Reward : 1.155, Train scores: 0.962244034734324, Test score : 0.977366255144033, policy lr: 0.0001\n",
      "Episode : 4580, Reward : 1.14325, Train scores: 0.9679222754778077, Test score : 0.983300198807157, policy lr: 0.0001\n",
      "Episode : 4600, Reward : 1.1415, Train scores: 0.9631453761989519, Test score : 0.9345637583892618, policy lr: 0.0001\n",
      "Episode : 4620, Reward : 1.166, Train scores: 0.9655318227075773, Test score : 0.9903575733226195, policy lr: 0.0001\n",
      "Episode : 4640, Reward : 1.1435, Train scores: 0.9752405029290104, Test score : 0.9692556634304207, policy lr: 0.0001\n",
      "Episode : 4660, Reward : 1.13825, Train scores: 0.9656324673062417, Test score : 0.9833679833679834, policy lr: 0.0001\n",
      "Episode : 4680, Reward : 1.0945, Train scores: 0.9616820551603664, Test score : 0.9914004914004914, policy lr: 0.0001\n",
      "Episode : 4700, Reward : 1.134, Train scores: 0.9748445732835762, Test score : 0.9687373122208689, policy lr: 0.0001\n",
      "Episode : 4720, Reward : 1.166, Train scores: 0.969110178491914, Test score : 0.9724166323589954, policy lr: 0.0001\n",
      "Episode : 4740, Reward : 1.1535, Train scores: 0.9693365717829986, Test score : 0.9963869931754316, policy lr: 0.0001\n",
      "Episode : 4760, Reward : 1.14775, Train scores: 0.9729265507626039, Test score : 0.9807928075194116, policy lr: 0.0001\n",
      "Episode : 4780, Reward : 1.1465, Train scores: 0.9664022985917766, Test score : 0.989289964299881, policy lr: 0.0001\n",
      "Episode : 4800, Reward : 1.165, Train scores: 0.9688438104635371, Test score : 0.9838842975206612, policy lr: 0.0001\n",
      "Episode : 4820, Reward : 1.1245, Train scores: 0.9613572365667288, Test score : 0.9868258542610128, policy lr: 0.0001\n",
      "Episode : 4840, Reward : 1.148, Train scores: 0.9659452030268495, Test score : 0.9799518845228549, policy lr: 0.0001\n",
      "Episode : 4860, Reward : 1.159, Train scores: 0.9693633316487643, Test score : 0.9704840613931524, policy lr: 0.0001\n",
      "Episode : 4880, Reward : 1.147, Train scores: 0.9693649838096168, Test score : 0.9847939175670268, policy lr: 0.0001\n",
      "Episode : 4900, Reward : 1.1265, Train scores: 0.9651238910493871, Test score : 0.984009840098401, policy lr: 0.0001\n",
      "Episode : 4920, Reward : 1.16825, Train scores: 0.9728911364599039, Test score : 0.9771380186282811, policy lr: 0.0001\n",
      "Episode : 4940, Reward : 1.15025, Train scores: 0.9678218137586594, Test score : 0.9711005542359462, policy lr: 0.0001\n",
      "Episode : 4960, Reward : 1.1515, Train scores: 0.9641543082308572, Test score : 0.9878048780487805, policy lr: 0.0001\n",
      "Episode : 4980, Reward : 1.18725, Train scores: 0.9679594947796332, Test score : 0.9927690344534241, policy lr: 0.0001\n"
     ]
    }
   ],
   "source": [
    "res = nomappo.learn(10000, update_frequency=20, test_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b04a135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9810219573912333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomappo.test(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a5831a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep: 0\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 1\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 2\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.3333,  0.5000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 3\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 4\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 5\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  0.5000,  0.3333,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 6\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 7\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 8\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.3333,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  0.5000,  0.3333,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 9\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 10\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 11\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  0.5000,\n",
      "         0.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 12\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.3333,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 13\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  0.3333,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 14\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 15\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 16\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.3333,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  0.5000,  0.5000,  0.3333,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 17\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 18\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 19\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.5000,  0.5000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 20\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 21\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  4.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 22\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 23\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.2500,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 24\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.2000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 25\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "Action: tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 26\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.3333,  1.0000,\n",
      "         0.5000,  0.5000,  0.5000,  0.5000,  1.0000,  0.3333,  0.5000,  0.5000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 27\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.2500,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 28\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  4.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 29\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 30\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000,  4.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 31\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.3333,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 32\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 33\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 34\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 35\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 36\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1.,  4., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 37\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 38\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 39\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 40\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  4.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 41\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  3.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  3.0000,  0.5000,  0.3333,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "        -1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 42\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [0. 0. 1. 1. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  2.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  2.0000,  1.0000,  0.2500,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "        -1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 43\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  2.0000,  0.5000,  0.2000,  1.0000,  0.5000,\n",
      "         0.5000,  0.3333,  0.5000,  1.0000,  0.5000,  0.3333,  0.3333,  1.0000,\n",
      "         2.0000]), Prior: tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
      "Action: tensor([0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 44\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.3333,  0.1667,  0.5000,  0.3333,\n",
      "         0.3333,  0.2500,  1.0000,  1.0000,  0.3333,  0.2500,  0.2500,  1.0000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 45\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 46\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  0.5000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 47\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 48\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  2.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 49\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  0.5000,  0.5000,\n",
      "         0.5000,  0.5000,  0.5000,  1.0000,  0.5000,  0.2500,  0.5000,  0.5000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 50\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.3333,  0.3333,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 51\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 52\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 53\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 54\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 55\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  4., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 56\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 57\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 58\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 59\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  0.5000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  0.5000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 60\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 61\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.3333,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 62\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1.,  4., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 63\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 64\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  4.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 65\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 66\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 67\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 68\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.,  4., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 69\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  4.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 70\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.3333,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 71\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 72\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 73\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 74\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  0.5000,\n",
      "         1.0000,  0.5000,  0.3333,  1.0000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 75\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 76\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  4.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "Action: tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 77\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  0.3333,  1.0000,  1.0000,\n",
      "         0.3333,  0.5000,  0.5000,  0.3333,  0.5000,  0.3333,  0.3333,  1.0000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 78\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 79\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  0.3333,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 80\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 81\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.3333,  0.5000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 82\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1.,  4., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 83\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 84\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  3.0000, -1.0000,  3.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "        -1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 85\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  2.0000, -1.0000,  2.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.5000,  0.3333,\n",
      "        -1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 86\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  2.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  0.5000,  0.3333,\n",
      "         1.0000,  1.0000,  0.3333,  1.0000,  0.5000,  0.5000,  0.3333,  0.2500,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 87\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.3333,  1.0000,  0.3333,  0.2500,\n",
      "         0.5000,  0.5000,  0.2500,  1.0000,  0.3333,  0.3333,  0.2500,  0.2000,\n",
      "         1.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 88\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1.]]\n",
      "obs: tensor([-1.0000,  3.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.2500,  1.0000,\n",
      "         0.3333,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "        -1.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 89\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]]\n",
      "obs: tensor([-1.0000,  3.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         3.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.2000,  0.5000,\n",
      "         1.0000,  0.5000,  0.5000,  0.5000,  1.0000,  1.0000,  0.5000,  0.5000,\n",
      "         2.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 90\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "obs: tensor([-1.0000,  2.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         2.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  0.1667,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  0.3333,  1.0000,  0.5000,  1.0000,  0.3333,\n",
      "        -1.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 91\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.1429,  1.0000,\n",
      "         0.3333,  0.5000,  0.5000,  0.2500,  1.0000,  0.3333,  0.5000,  0.2500,\n",
      "         3.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 92\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.1250,  1.0000,\n",
      "         0.2500,  1.0000,  0.3333,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 93\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         0.2000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 94\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 95\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000,  4.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 96\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1.,  4., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 97\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 98\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  0.5000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 99\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 100\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.3333,  0.3333,  1.0000,\n",
      "         0.5000,  0.5000,  0.3333,  0.5000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 101\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1.,  4., -1.,  4., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 102\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  4.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 103\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 104\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 105\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.3333,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  0.2500,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "Action: tensor([1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 106\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.3333,  0.2500,  1.0000,\n",
      "         0.5000,  0.3333,  0.5000,  0.2000,  1.0000,  0.5000,  0.5000,  0.5000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 107\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 108\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  2.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  0.5000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 109\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1.,  1.,  3., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.]), Prior: tensor([0.6000, 0.6000, 1.0000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 110\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 111\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  3.]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 112\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "        -1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 113\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  0.5000,  0.5000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  0.3333,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 114\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 1. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1.,  3., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1., -1.]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 115\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 116\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  0.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 117\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  0.5000,  1.0000,  0.5000,\n",
      "         0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  0.3333,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 118\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  0.3333,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 119\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 120\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  4.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 121\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 122\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 123\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 124\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  0.5000,  0.3333,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 125\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  4.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 126\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  4.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 127\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 128\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  4.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 129\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  0.5000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 130\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  0.5000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 131\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 132\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 133\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 134\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  4.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 135\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 136\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  4.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 137\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1.,  4., -1.,  4., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 138\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1.,  4., -1., -1.,  4., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 139\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 140\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 141\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         0.5000,  1.0000,  0.3333,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 142\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000, -1.0000,  4.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
      "Action: tensor([0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 143\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "         1.0000,  0.5000,  0.5000,  0.3333,  1.0000,  0.3333,  0.5000,  1.0000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 144\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1.,  4.,  4., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        1.0000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 145\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 146\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 147\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 148\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 149\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.3333,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 150\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.3333,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 151\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 152\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 153\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 154\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 155\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1.,  4., -1.,  4., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 1.0000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 156\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  0.5000,  1.0000,  0.5000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 157\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 158\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000,  4.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 159\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  0.5000,\n",
      "         0.5000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 160\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1.,  4., -1., -1.,  4., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 161\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 162\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 163\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 164\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  0.3333,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 165\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 166\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 167\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 168\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 169\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  0.5000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 1.0000, 0.6000])\n",
      "Action: tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 170\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  0.5000,\n",
      "         1.0000,  1.0000,  1.0000,  0.3333,  1.0000,  1.0000,  1.0000,  0.3333,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 171\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([ 4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 172\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.3333,  0.5000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  0.3333,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 173\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  1.0000,  0.5000,  0.2500,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 174\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  0.5000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 175\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.3333,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 176\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  4.0000, -1.0000, -1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 0.6000])\n",
      "Action: tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 177\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 178\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 179\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 180\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.3333,  0.5000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 181\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  0.5000,\n",
      "         1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 182\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  4.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  0.3333,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.3333,  0.5000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 183\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000,  4.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000,  4.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  0.5000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Action: tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 184\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  1.0000,  0.5000,\n",
      "         0.5000,  0.3333,  0.3333,  0.5000,  0.5000,  0.5000,  1.0000,  0.5000,\n",
      "         3.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 185\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 186\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000,  4.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 187\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n",
      "obs: tensor([-1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,  1.,  1.,\n",
      "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 188\n",
      "Current state: [[0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  0.5000,  0.5000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "        -1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 189\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  3.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 190\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  4.0000, -1.0000,  3.0000,  0.5000,  0.3333,  1.0000,  1.0000,\n",
      "         0.3333,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 191\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 1.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000,  3.0000, -1.0000,  2.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,\n",
      "        -1.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        1.0000, 0.6000, 1.0000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.])\n",
      "reward: -1\n",
      "\n",
      "\n",
      "Timestep: 192\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 1. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  2.0000,\n",
      "        -1.0000,  2.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "        -1.0000]), Prior: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.])\n",
      "Action: tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1.])\n",
      "reward: 3.0\n",
      "\n",
      "\n",
      "Timestep: 193\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000,  2.0000,  0.5000,  0.5000,  0.5000,  0.5000,\n",
      "         0.3333,  0.5000,  0.3333,  1.0000,  0.5000,  1.0000,  0.5000,  1.0000,\n",
      "         3.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 1.0000])\n",
      "Action: tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n",
      "Timestep: 194\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.3333,  0.3333,  0.3333,  0.3333,\n",
      "         0.2500,  0.3333,  1.0000,  0.5000,  0.3333,  0.5000,  0.3333,  1.0000,\n",
      "         1.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 195\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 196\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  0.5000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  0.5000,  0.5000,  0.5000,  0.5000,  1.0000,\n",
      "         0.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 197\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  0.5000,  1.0000,  0.3333,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,\n",
      "         2.0000]), Prior: tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Action: tensor([1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 0.0\n",
      "\n",
      "\n",
      "Timestep: 198\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "         4.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         1.0000,  1.0000,  1.0000,  1.0000,  1.0000,  0.5000,  1.0000,  1.0000,\n",
      "         0.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000, 1.0000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1.])\n",
      "reward: 2.0\n",
      "\n",
      "\n",
      "Timestep: 199\n",
      "Current state: [[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]]\n",
      "obs: tensor([-1.0000, -1.0000, -1.0000,  4.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
      "        -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  1.0000,  1.0000,  1.0000,\n",
      "         0.5000,  1.0000,  0.5000,  1.0000,  1.0000,  0.3333,  0.5000,  1.0000,\n",
      "         2.0000]), Prior: tensor([0.6000, 0.6000, 0.6000, 1.0000, 0.6000, 0.6000, 0.6000, 0.6000, 0.6000,\n",
      "        0.6000, 0.6000, 0.6000])\n",
      "Action: tensor([1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.])\n",
      "reward: 1.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9529411764705882"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nomappo.test(1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b14b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65f164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3554cced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
